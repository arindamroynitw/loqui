//
//  AppState.swift
//  Loqui
//
//  Created by Arindam Roy on 31/12/25.
//

import Foundation
import SwiftUI

/// Central state machine for the Loqui app
/// This orchestrates the entire speech-to-text pipeline
@MainActor
class AppState: ObservableObject {
    // MARK: - Singleton
    static let shared = AppState()

    // MARK: - State Enum
    enum State {
        case idle
        case recording(startTime: Date)
        case processing
        case error(Error)
    }

    // MARK: - Published Properties
    @Published var currentState: State = .idle
    @Published var statusText: String = "Idle"
    @Published var currentModel: String = "Not loaded"
    @Published var modelsReady: Bool = false  // Track model initialization state

    // MARK: - Computed Properties
    var hasAPIKeys: Bool {
        // Check UserDefaults directly to avoid race condition with async initialization
        let groqKey = UserDefaults.standard.string(forKey: "groqAPIKey") ?? ""
        let openaiKey = UserDefaults.standard.string(forKey: "openaiAPIKey") ?? ""
        return !groqKey.isEmpty || !openaiKey.isEmpty
    }

    // MARK: - Core Components
    private var fnKeyMonitor: FnKeyMonitor?

    // MARK: - Audio Pipeline (Phase 2)
    private var audioCapture: WhisperAudioCapture?
    private var audioBuffer: Data = Data()

    // MARK: - Transcription (Phase 3)
    private var transcriptionEngine: TranscriptionEngine?

    // MARK: - LLM (Phase 4 - Cloud API)
    private var groqClient: GroqClient?
    private var openaiClient: OpenAIClient?

    // MARK: - Text Insertion (Phase 5)
    private var textInserter: TextInserter?
    private var hudController: HUDWindowController?

    // MARK: - Initialization
    private init() {
        print("üöÄ AppState: Initializing...")
        // Note: HUD is created lazily to avoid circular dependency
        // (HUDViewModel observes AppState.shared, so can't create during init)
    }

    // MARK: - fn Key Monitoring
    func startKeyMonitoring() {
        // Initialize HUD now (after AppState.shared initialization complete)
        // This avoids circular dependency that would occur in init()
        if hudController == nil {
            hudController = HUDWindowController()
            print("‚úÖ AppState: HUD initialized")
        }

        fnKeyMonitor = FnKeyMonitor()
        fnKeyMonitor?.start()

        // Listen for fn key press events
        NotificationCenter.default.addObserver(
            forName: .fnKeyPressed,
            object: nil,
            queue: .main
        ) { [weak self] _ in
            self?.startRecording()
        }

        // Listen for fn key release events
        NotificationCenter.default.addObserver(
            forName: .fnKeyReleased,
            object: nil,
            queue: .main
        ) { [weak self] _ in
            self?.stopRecording()
        }

        print("‚úÖ AppState: fn key monitoring started")
    }

    // MARK: - Recording Control
    func startRecording() {
        // Block if already processing
        guard case .idle = currentState else {
            print("‚ö†Ô∏è  AppState: Cannot start recording - currently \(currentState)")
            // TODO: Show notification "Still processing previous transcription"
            return
        }

        // Check if any API key is configured
        if !hasAPIKeys {
            print("‚ö†Ô∏è  AppState: No API keys configured")
            showAPIKeyRequiredAlert()
            return
        }

        // Check if models are ready (prevents 131s hang on first transcription)
        if !modelsReady {
            print("‚ö†Ô∏è  AppState: Models not ready yet, showing loading HUD")
            Task { await setHUDLoadingState() }
            return
        }

        currentState = .recording(startTime: Date())
        statusText = "Recording..."
        print("üé§ AppState: Recording started")

        // Phase 2: Initialize audio capture
        audioBuffer = Data()
        audioCapture = WhisperAudioCapture()
        audioCapture?.onAudioChunk = { [weak self] chunk in
            self?.audioBuffer.append(chunk)
        }

        do {
            try audioCapture?.startCapture()
        } catch {
            LoquiLogger.shared.logError(error, context: "Audio capture start")
            print("‚ùå AppState: Failed to start audio capture: \(error)")

            // Return to idle so user can try again
            DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) { [weak self] in
                self?.currentState = .idle
                self?.statusText = "Idle"
            }

            currentState = .error(error)
            statusText = "Error"
            return
        }

        // Phase 5: Initialize text inserter
        if textInserter == nil {
            textInserter = TextInserter()
        }

        // Floater automatically updates via state observation
    }

    func stopRecording() {
        guard case .recording = currentState else {
            return
        }

        print("üõë AppState: Recording stopped")
        print("üìä AppState: Audio buffer size: \(audioBuffer.count) bytes")

        // Phase 2: Stop audio capture
        audioCapture?.stopCapture()

        // Floater automatically updates via state observation

        // Transition to processing state
        currentState = .processing
        statusText = "Processing transcription..."

        // Phase 2: Process recording
        Task {
            await processRecording()
        }
    }

    // MARK: - Model Initialization (Phase 3 & 4)
    func initializeModels() async {
        statusText = "Loading models..."
        print("üì¶ AppState: Initializing models...")

        // Show HUD in loading state (visible to user during 131s model load)
        await setHUDLoadingState()

        do {
            // Phase 3: Initialize Whisper
            transcriptionEngine = TranscriptionEngine()
            try await transcriptionEngine?.initialize()

            currentModel = "distil-large-v3 (594MB)"
            print("‚úÖ AppState: Whisper model loaded")

            // Phase 4: Initialize Cloud LLM
            var llmProviders: [String] = []

            if let apiKey = UserDefaults.standard.string(forKey: "groqAPIKey"), !apiKey.isEmpty {
                groqClient = GroqClient(apiKey: apiKey)
                llmProviders.append("Groq")
                print("‚úÖ AppState: Groq client initialized")
            } else {
                print("‚ö†Ô∏è  AppState: No Groq API key configured")
            }

            if let apiKey = UserDefaults.standard.string(forKey: "openaiAPIKey"), !apiKey.isEmpty {
                openaiClient = OpenAIClient(apiKey: apiKey)
                llmProviders.append("OpenAI")
                print("‚úÖ AppState: OpenAI client initialized")
            } else {
                print("‚ö†Ô∏è  AppState: No OpenAI API key configured")
            }

            if !llmProviders.isEmpty {
                currentModel = "Whisper + \(llmProviders.joined(separator: " + "))"
            } else {
                currentModel = "Whisper (No LLM - Configure API key in Settings)"
            }

            // Mark models as ready
            modelsReady = true
            print("‚úÖ AppState: All models loaded and ready")

        } catch {
            LoquiLogger.shared.logError(error, context: "Model initialization")
            print("‚ùå AppState: Model initialization failed: \(error)")
            currentModel = "Model load failed"
            modelsReady = false  // Keep as not ready on failure
        }

        // Hide HUD after loading complete
        await clearHUDLoadingState()

        statusText = "Idle"
    }

    // MARK: - Processing Pipeline (Phase 2-5)
    private func processRecording() async {
        let pipelineStart = Date()
        print("‚è±Ô∏è  [0.00s] Pipeline started")

        do {
            // Phase 2: VAD analysis
            let vadStart = Date()
            let vadProcessor = try await VADProcessor()
            let vadResult = try await vadProcessor.analyzeRecording(audioBuffer)
            let vadTime = Date().timeIntervalSince(vadStart)
            print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] VAD complete (\(String(format: "%.2f", vadTime))s)")

            guard case .speech(let trimmedAudio) = vadResult else {
                print("‚ùå AppState: No speech detected")
                currentState = .idle
                statusText = "Idle"
                // TODO Phase 5: Show notification "No speech detected"
                return
            }

            print("‚úÖ AppState: Speech detected, \(trimmedAudio.count) bytes after trimming")

            // Phase 3: Transcription
            let whisperStart = Date()
            print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] Whisper transcription started")
            guard let rawText = try await transcriptionEngine?.transcribe(trimmedAudio) else {
                throw TranscriptionError.notInitialized
            }
            let whisperTime = Date().timeIntervalSince(whisperStart)
            print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] Whisper complete (\(String(format: "%.2f", whisperTime))s)")
            print("üìù Transcription: '\(rawText)'")

            // Phase 4: Cloud LLM cleanup (with OpenAI fallback)
            let llmStart = Date()
            print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] LLM cleanup started")
            var finalText = rawText
            var cleanupFailed = false
            var usedProvider: String?

            // Try Groq first (primary provider)
            if let groqClient = groqClient {
                do {
                    finalText = try await groqClient.cleanTranscript(rawText)
                    let llmTime = Date().timeIntervalSince(llmStart)
                    print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] LLM complete (Groq: \(String(format: "%.2f", llmTime))s)")
                    print("‚ú® LLM Cleaned: '\(rawText)' ‚Üí '\(finalText)'")
                    usedProvider = "Groq"
                } catch let error as LLMError {
                    let llmTime = Date().timeIntervalSince(llmStart)
                    print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] Groq failed (\(String(format: "%.2f", llmTime))s)")
                    print("‚ö†Ô∏è  Groq cleanup failed: \(error.localizedDescription)")

                    // Try OpenAI fallback
                    if let openaiClient = openaiClient {
                        print("üî∑ Attempting OpenAI fallback...")
                        let openaiStart = Date()
                        do {
                            finalText = try await openaiClient.cleanTranscript(rawText)
                            let openaiTime = Date().timeIntervalSince(openaiStart)
                            print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] LLM complete (OpenAI: \(String(format: "%.2f", openaiTime))s)")
                            print("‚ú® LLM Cleaned (OpenAI fallback): '\(rawText)' ‚Üí '\(finalText)'")
                            usedProvider = "OpenAI"
                        } catch {
                            print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] OpenAI also failed")
                            print("‚ö†Ô∏è  Both providers failed, using raw transcription")
                            cleanupFailed = true
                        }
                    } else {
                        print("‚ö†Ô∏è  No OpenAI fallback available, using raw transcription")
                        cleanupFailed = error.shouldShowOverlay
                    }
                } catch {
                    let llmTime = Date().timeIntervalSince(llmStart)
                    print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] Groq failed (\(String(format: "%.2f", llmTime))s)")
                    print("‚ö†Ô∏è  Groq cleanup failed: \(error)")

                    // Try OpenAI fallback
                    if let openaiClient = openaiClient {
                        print("üî∑ Attempting OpenAI fallback...")
                        let openaiStart = Date()
                        do {
                            finalText = try await openaiClient.cleanTranscript(rawText)
                            let openaiTime = Date().timeIntervalSince(openaiStart)
                            print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] LLM complete (OpenAI: \(String(format: "%.2f", openaiTime))s)")
                            print("‚ú® LLM Cleaned (OpenAI fallback): '\(rawText)' ‚Üí '\(finalText)'")
                            usedProvider = "OpenAI"
                        } catch {
                            print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] OpenAI also failed")
                            print("‚ö†Ô∏è  Both providers failed, using raw transcription")
                            cleanupFailed = true
                        }
                    } else {
                        print("‚ö†Ô∏è  No OpenAI fallback available, using raw transcription")
                        cleanupFailed = true
                    }
                }
            } else if let openaiClient = openaiClient {
                // No Groq, try OpenAI directly
                print("‚ö†Ô∏è  No Groq client configured, trying OpenAI...")
                do {
                    finalText = try await openaiClient.cleanTranscript(rawText)
                    let llmTime = Date().timeIntervalSince(llmStart)
                    print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] LLM complete (OpenAI: \(String(format: "%.2f", llmTime))s)")
                    print("‚ú® LLM Cleaned: '\(rawText)' ‚Üí '\(finalText)'")
                    usedProvider = "OpenAI"
                } catch {
                    print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] OpenAI failed")
                    print("‚ö†Ô∏è  OpenAI cleanup failed, using raw transcription")
                    cleanupFailed = true
                }
            } else {
                let llmTime = Date().timeIntervalSince(llmStart)
                print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] LLM skipped (no API keys) (\(String(format: "%.2f", llmTime))s)")
                print("‚ö†Ô∏è  No LLM clients configured, using raw transcription")
            }

            // Phase 5: Insert text
            let insertStart = Date()
            do {
                try textInserter?.insertText(finalText)
                let insertTime = Date().timeIntervalSince(insertStart)
                print("‚è±Ô∏è  [\(String(format: "%.2f", Date().timeIntervalSince(pipelineStart)))s] Text insertion complete (\(String(format: "%.3f", insertTime))s)")
                print("‚úÖ AppState: Text inserted successfully")
            } catch {
                LoquiLogger.shared.logError(error, context: "Text insertion")
                print("‚ùå AppState: Text insertion failed: \(error)")
                // TODO Phase 5: Show error notification to user
                // For now, just log the error
            }

            // Note: LLM cleanup failures are handled by HUD error state
            // Raw transcription already inserted above, error shown in HUD for 2s

            let totalTime = Date().timeIntervalSince(pipelineStart)
            print("‚è±Ô∏è  ‚è±Ô∏è  ‚è±Ô∏è  TOTAL PIPELINE LATENCY: \(String(format: "%.2f", totalTime))s")
            print("‚úÖ AppState: Phase 5 complete - final text: '\(finalText)'")

        } catch {
            LoquiLogger.shared.logError(error, context: "Recording processing")
            print("‚ùå AppState: Processing error: \(error)")
            currentState = .error(error)
            statusText = "Error"

            // TODO Phase 5: Show error notification to user

            // Return to idle after 2 seconds so user can try again
            try? await Task.sleep(nanoseconds: 2_000_000_000)
        }

        // Always return to idle (either after success or after error delay)
        currentState = .idle
        statusText = "Idle"
        print("üîÑ AppState: Returned to idle state")
    }

    // MARK: - Helper Functions
    private func showAPIKeyRequiredAlert() {
        DispatchQueue.main.async {
            let alert = NSAlert()
            alert.messageText = "API Key Required"
            alert.informativeText = "Please configure your Groq or OpenAI API key in Settings (Cmd+,) before using Loqui."
            alert.alertStyle = .informational
            alert.addButton(withTitle: "OK")
            alert.runModal()
        }
    }

    // MARK: - HUD Loading State Management
    /// Show HUD in loading state (called during model initialization)
    private func setHUDLoadingState() async {
        await MainActor.run {
            hudController?.setLoadingState()
            hudController?.show()
            print("‚úÖ AppState: HUD set to loading state")
        }
    }

    /// Clear HUD loading state and return to waiting (called after model initialization)
    private func clearHUDLoadingState() async {
        await MainActor.run {
            // Don't hide - transition to waiting state (ambient presence)
            hudController?.setWaitingState()
            print("‚úÖ AppState: HUD loading state cleared, returned to waiting")
        }
    }

    // MARK: - Cleanup
    func cleanup() {
        fnKeyMonitor?.stop()
        audioCapture?.stopCapture()
        print("üßπ AppState: Cleanup complete")
    }
}
